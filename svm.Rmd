---
title: "SVM"
output: html_document
date: "2023-12-22"
---
```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

# Метод опорных векторов SVM 

## Задача обучения линейного классификатора

**Дано:**<br>
**Обучающая выборка** $X^l=(x_i, y_i)_{i=1}^l,$<br>
$x_i$ --- **объекты**, векторы из множества $X = \mathbb{R}^n,$<br>
$y_i$ --- **метки классов**, элементы множества $Y={-1,+1}$ (задача классификация с двумя классами).<br>
Будем строить **линейный классфификатор**. То есть нужно **найти**:<br>
Параметры $w\in \mathbb{R}^n, ~~ w_0\in \mathbb{R}$ линейной модели классификации
$$a(x; w, w_0)=\text{sign} \Big(\langle x, w \rangle - w_0\Big).$$
То есть это скалярное произведение $x$ на вектор весов, и мы берем знак этого скалряного произведения --- то есть если скалярное произведение положительно, то объект относится к классу +1, если отрицательно, то -1. Однако тут еще есть свободный член, с которым будем сравнивать скалярное произведение.<br>
**Исходный критерий** --- минимизация эмпирического риска(число ошибок на обучающей выборке):
$$\sum_{i=1}^l[a(x; w, w_0)\neq y_i]=\sum_{i=1}^l[M_i(w, w_0)<0]\to \min_{w, w_0}, $$
где $M_i(w, w_0) = \Big(\langle x, w \rangle - w_0\Big)y_i$ --- отступ(margin) объекта $x_i$ (То есть расстояние от объекта до разделяющей гиперплоскости, причем расстояние со знаком).
Теперь возьмем конкретную аппроксимацию пороговой функции потерь --- это кусочно линейная функция $1-$отступ с положительной срезкой, а исходная пороговая функция потерь изображена синей. Значения потерь как функцию от отступа. Ясно, что чем больше отступ --- тем лучше, поэтому в задачах классификации выгодно брать такие функции потерь, чтобы они не возрастали.

**Эмпирический риск** --- кусочно-постоянная функция. Заменим ее оценкой сверху, непрерывной по параметрам:
$$\begin{aligned}
Q(w, w_0) &= \sum_{i=1}^l\color{red}{[M_i(w, w_0)<0]}\leqslant\\
&\leqslant 
\sum_{i=1}^l\color{blue}{\Big(1 - M_i(w, w_0)<0\Big)_+} +\frac{1}{2C}\|w\|^2\to \min_{w, w_0}
\end{aligned}
$$
![](C:/Users\\yanas\\Documents\\7R\\approx.jpg) 



* Аппроксимация штрафует объекты за приближение к границе классов, увеличивая зазор между классами<br>
* Регуляризация штрафует неустойчивые решения в случае мультиколлинераности (квадаратичный регуляризатор помогает справиться с проблемами мультиколлинеарности(линейная комбинация/почти лин комб) и переобучения)<br>

Эти два принципа (пороговая функция потерь и регуляризация) приводят к такой постановки задачи,которая называется методом опорных векторов, решая эту задачу оптимизации мы будем применять метод опорных векторов.<br>

## Другой принцип постановки задачи --- **принцип оптимальной разделяющей гиперплоскости**
Рассмотрим линейный классификатор:$a(x; w, w_0)=\text{sign} \Big(\langle x, w \rangle - w_0\Big).$<br>

Пусть выборка $X^l=(x_i, y_i)_{i=1}^l$ линейно разделима:
$$\exists w, w_0:~~ M_i(w, w_0) = y_i\Big(\langle x_i, w \rangle - w_0\Big)>0, ~~i\in 1:l$$


То есть все отступы строго положительны и отделены от нуля, тогда нет ошибок на обучающей выборке, то есть она линейно разделима. В силу линейности модели, эта система неравенств определена с точностью жо нормировки. То есть можно левую часть неравенст умножить на какую-нибудь константу и отэтого ничего не изменится. Тогда выберем нормировочный коэффициент так, чтобы минимальный отступ был равен единице:
$$\min_{i\in 1:l}~M_i(w, w_0) = 1.$$

Также введем такой принцип: попытаемся провести разделяющие гиперплоскости таким образом, чтобы полоса была как можно шире(классификация будет надежнее --- эвристический принцип). Если полоса максимально широкая, то легкго доказать, что своими краями(границами) Полоса обязательно упрется в какие-то объекты обучающе1 выборки, причем с одной стороны в объекты класса -, с другой --- в объекты класса +. Это означает, что на объектах, в которые она уперлась отступ в точности равен единице.
То есть разделяющая полоса --- геометрическое место точек, удовлетворящее этому неравенству:
$$\{x: -1\leqslant \langle x, w \rangle - w_0\leqslant 1 \},$$
причем
$$\exists x_+:~~\langle x_+, w \rangle - w_0=+1,\\
\exists x_-:~~\langle x_-, w \rangle - w_0=-1.$$

![](C:/Users\\yanas\\Documents\\7R\\polosa.jpg) 

#### **Ширина разделяющей полосы(разность двух векторов проецируем на нормаль, которая должна быть нормированная, чтобы мы получили длину):**

$$\frac {\langle x_+ - x_-, w \rangle}{\|w\|}= \frac{2}{\|w\|}\to \max$$
То есть максимизация длиня полосы --- это минимизация нормы $w$. Отсюда понятен смысл регуляризатора в данной задаче.<br>

### **Постановка задачи квадратичного программирования в линейно-разделимом случае:**

$$\begin{cases}
1/2 \|w\|^2\to \min_{w, w_0};\\
M_i(w, w_0)\geqslant 1, ~~i\in 1:l
\end{cases}$$


То есть минимизируем квадратичный по $w$ функционал при линеных ограничениях и неравенствах.<br>

Однако в общем случае, когда выборка линейно неразделима, то система несовместна. Ослабим условия: введем в эти неравенства дополнительные положительные переменные, характеризующие величину ошибки на объектах(штраф за нарушение неравенства). Ослабим ограничения-неравенства и одновременно введём в минимизируемый функционал штраф за суммарную ошибку:
$$\begin{cases}
1/2 \|w\|^2 + C\sum_{i=1}^l \xi_i\to \min_{w, w_0, \xi};\\
M_i(w, w_0)\geqslant 1 - \xi_i, ~~i\in 1:l;\\
\xi_i\geqslant 0, ~~i\in 1:l.
\end{cases}$$<br>

Эта система всегда совместна (можем подобрать $\xi_i$).
Тогда несложно сформулировать **эквивалентную задачу безусловной минимизации**:
$$1/2 \|w\|^2 + C\sum_{i=1}^l \Big(1 - M_i(w, w_0)\Big)_+\to \min_{w, w_0}$$

#### **Условия Каруша-Куна-Такера:**
$$
\begin{cases}
f(x)\to \min_{x};\\
g_i(x)\leqslant 0, ~~ i\in 1:m;\\
h_i(x)= 0, ~~ j\in 1:k.
\end{cases}
$$


#### **Применение условий Каруша-Куна-Такера к задаче SVM:**
Чтобы решить такую задачу, нужно выписать Лагранжиан --- линейную комбинацию функционала, который минимизируем и всех ограничений(равенств и неравенств):
$$
\begin{cases}
\frac{\partial\cal{L}}{\partial x} = 0, ~~\cal{L}(x; \mu, \lambda)=f(x)+\sum \limits_{i=1}^m
\mu_i g_i(x)+\sum \limits_{j=1}^k
\lambda_j h_j(x);\\
g_i(x)\leqslant 0; ~~h_j(x)=0; ~~~\text{(исходные ограничения)}\\
\mu_i\geqslant 0;~~~\text{(двойственные ограничения)}\\
\mu_i g_i(x)=0;~~~\text{(уcловие дополняющей нежесткости)}
\end{cases}
$$

Итак, выпишем функцию Лагранжа(немного перегруппировав слагаемые):
$$\cal{L}(w, w_0, \xi; \lambda, \eta)=\frac 12 \|w\|^2-\sum_{i=1}^l\Big(М_i(w, w_0)- 1 \Big) -\sum_{i=1}^l\xi_i\Big(\lambda_i+\eta_i-С\Big),$$

где $\lambda_i$ --- переменные, двойственные к ограничениям $M_i\geqslant 1-\xi_i;$<br>
$\eta_i$ --- переменные, двойственные к ограничениям $\xi_i\geqslant0.$

Запишем условия:

$$
\begin{cases}
\frac{\partial \cal{L}}{\partial w} = 0,~~
\frac{\partial \cal{L}}{\partial w_0} = 0,~~
\frac{\partial \cal{L}}{\partial \xi} = 0;\\
\xi_i\geqslant 0,  ~~ \lambda_i\geqslant 0, ~~\eta_i\geqslant 0, ~~i\in 1:l;\\
\lambda_i=0~~\text{либо}~~M_i(w, w_0)=1-\xi_i, ~~i\in 1:l;\\
\eta_i=0~~\text{либо}~~\xi_i=0, ~~i\in 1:l.
\end{cases}
$$
Легко проверить, что из условий на проивзодные получаем:
$$
w=\sum_{i=1}^l\lambda_i y_i x_i;~~\sum_{i=1}^l\lambda_i y_i=0;~~\eta_i+\lambda_i=C, ~i\in 1:l.
$$<br>
Получаем, чесу равен вектор решения, который мы ищем. То есть этот вектор есть линейная комбинация объектов обучающей выборки, причем не всех(так как $\lambda_i$ может принимать нулевые значения). Те объекты, от которых решение зависит, называются **опорными векторами**.<br>

#### **Понятие опорного вектора**
Из всех условий, которые есть, можно понять, что объекты делятся на три типа по тому, какое значение принимает двойственная переменная $\lambda_i$:

1. $\lambda_i=0;~~ \eta_i=C;~~ \xi_i=0;~~ M_i\geqslant 1$ --- периферийные (неинформативные) объекты. (то есть $\lambda_i=0$ --- решение не зависит от этого объекта, отсуп больше или равен 1 означает, что на этом объекте нет ошибкиЖ он "лежит глубоко в толще" своего класса(либо на границе)).

2. $0<\lambda_i<C;~~ 0<\eta_i<C;~~ \xi_i=0;~~ M_i = 1$ --- ***опорные граничные объекты***.

3. $\lambda_i=С;~~ \eta_i=0;~~ \xi_i>0;~~ M_i<1$ --- ***опорные-нарушители***(То есть либо они попадают в разделяющую полосу, либо находся за ее пределами в "чужом" классе). Это объекты, на которых ошибка может быть(отступ меньше нуля --- попали в другой класс) или не быть(если попали в полосу --- зазор между классами).


#### **Определение:**
Объект $x_i$ называется ***опорным***, если $\lambda_i \neq 0$.

Вернемся к системе и применим все те ограничения, которые уже выписали, задавшись целью избавить от всех перменных, кроме двойственных. То сможем записать задачу квадартичного программирования так:
$$
\begin{cases}
\cal{L}(\lambda) = -\sum_{i=1}^l \lambda_i +\frac 12 \sum_{i=1}^l \sum_{j=1}^l \lambda_i \lambda_j y_i y_J \langle x_i, x_j\rangle \to \min_{\lambda};\\

0\leqslant \lambda_i\leqslant C, ~~i\in 1:l;\\
\sum_{i=1}^l \lambda_i y_i=0;
\end{cases}
$$

Ограничения равенства и неравенства определяют куб в пространстве размерности l с ребром С. Этот куб рассечен гиперплоскостью(ограничение неравенства). Пересечение куба и гиперплоскости --- это есть то множество в пространстве \lambda, на котором мы ищем решения. То есть задача выпуклая. Н выпуклой области ищем минимум выпуклого квадратичного функционала, то есть решение гарантировано должно быть единственным.

Решение прямой задачи выражается через решение двойственной:
$$
\begin{cases}
w=\sum_{i=1}^l \lambda_i y_i x_i;\\
w_0=\langle w, x_i\rangle - y_i, ~~~\forall i:~~\lambda_i>0,~M_i=1.
\end{cases}
$$
Получим линейный классификатор с признаками $f_i(x) = \langle x, x_i\rangle$:

$$a(x)=\text{sign}\Big(\sum_{i=1}^l \lambda_i y_i\langle x, x_i\rangle - w_0\Big).$$

#### **Влияние константы С на решение SVM**
SVM --- аппроксимация и регуляризация эмпирического риска
$$\sum_{i=1}^l\Big(1 - M_i(w, w_0)<0\Big)_+ +\frac{1}{2C}\|w\|^2\to \min_{w, w_0}.$$
![Влияние константы С](C:/Users\\yanas\\Documents\\7R\\reg.jpg) 


Возникает вопрос,можно ли в постановке задачи заменить скалярное произведение просто какой-то функцией от пары объектов.<br>

## **Нелинейное обощение SVM**

При наличии нелинейной связи между признаками и откликом качество линейных классификаторов часто может оказаться неудовлетворительным. Для учета нелинейности обычно расширяют пространство переменных, включая различные функциональные преобразования исходных предикторов (полиномы, экспоненты и проч.). Машину опорных векторов SVM (Support Vector Machine) можно рассматривать как нелинейное обобщение линейного классификатора, основанное на расширении размерности исходного пространства предикторов с помощью специальных ядерных функций. Это позволяет строить модели с использованием разделяющих поверхностей самой различной формы.<br>


**Идея** заменить $\langle x, x'\rangle$ нелинейной функцией $K(x, x')$. Это приводит к идее спрямляющего пространства, как правило, более высокой размерности: воспользуемся какой-нибудь функцией  $\psi:X\to H$ (в новом пространстве существует скалярное произведение и мы будем пользоваться им).

### **Определение**
Функция $K:X\times X\to \mathbb{R}$ --- ядро, если $K(x,x')=\langle\psi(x), \psi(x')\rangle$ при некотором $\psi:X\to H$, где $H$--- гильбертово пространство.

### **Теорема**
Функция $K(x,x')$ является ядром тогда и только тогда, когда она симметрична: $K(x, x') = K(x', x)$; и неотрицательно определена:
$$\int_X\int_X K(x, x')g(x) g(x')dxdx'\geqslant 0 ~~~\forall g:X\to \mathbb{R}.$$

### **Конструктивные методы синтеза ядер:**

1. $K(x, x') = \langle x, x'\rangle$ --- ядро;

2. Константа  $K(x, x') = 1$ --- ядро;

3. Произведение ядер  $K(x, x') = K_1(x, x')K_2(x, x')$ --- ядро;

4. $\forall \psi: X\to \mathbb{R}$ произведение $K(x, x') = \psi(x)\psi(x')$ --- ядро;

5.  $K(x, x') = \alpha_1 K_1(x, x')+\alpha_2 K_2(x, x'), ~~~\alpha_1, \alpha_2>0$ --- ядро.

6. $\forall \varphi: X\to X$, если $K_0$ ядро, то  $K(x, x')=K_0(\varphi(x), \varphi(x')$--- ядро;

7. Если $s:X\times X\to \mathbb{R}$ --- симметричная интегрируемая функция, то $K(x, x') = \int_X s(x,z)s(x',z)dz$ --- ядро;

8. Если $R_0$ --- ядро и функця $f:\mathbb{R}\to \mathbb{R}$ представима в виде сходящегося степенного ряда с неотрицательными коэффициентами, то $K(x, x')=F\Big(K(x, x')\Big)$ --- ядро.


#### **Примеры ядер**

1. $K(x, x') = \langle x, x'\rangle^2$ --- квадратичное ядро

2. $K(x, x') = \langle x, x'\rangle^d$ --- полиномиальное ядро с мономами степени $d$

3. $K(x, x') = \Big(\langle x, x'\rangle +1\Big)^d$ --- полиномилаьное ядро с мономами степени  $\leqslant d$ 

4. $K(x, x') = th\Big(k_1\langle x, x'\rangle -k_0\Big)^d$,  $k_0, k_1\geqslant0$ --- нейросеть с сигмоидными функциями активации

5. $K(x, x') = exp\{-\gamma\|x-x'\|^2\}$ --- сеть радиальных базисных функций (RBF ядро)

## Пример с использованием R


```{r}
library(readxl)
library(tidyverse)
library(kableExtra)
library(ggplot2) # графики
library(kernlab) # Support Vector Machines
library(dplyr) # манипуляции с данными
library(caret) # построение моделей

data <- read.table(file = "svmdata5.txt")
dataTest <- read.table(file = "svmdata5test.txt")

print_df <- function(df)
{
  df |>
    kable(format = "html") |>
    kable_styling() |>
    kableExtra::scroll_box(width = "100%", height = "100%")
}

head(data, 8) |>
 print_df()
```

#### Обучающая и тестовая выборки:
```{r}
train_data <- data
test_data <- dataTest
```
 

#### Для ksvm нужно указать, что зависимая переменная — факторная, а не количественная:
```{r}
train_data$Colors <- as.factor(ifelse(train_data$Colors == "red", 0, 1))
test_data$Colors <- as.factor(ifelse(test_data$Colors == "red", 0, 1))
```


#### Посмотрим, как распределны данные (отделимы линейно?)
```{r}

library(ggplot2)


ggplot(data, aes(x = X1, y = X2, color = Colors)) +
  geom_point() +
  labs(title = "Диаграмма рассеяния X1 vs X2", x = "X1", y = "X2", color = "Color") +
  scale_color_manual(values = c("red", "green")) 

```


Находим оптимальную разделяющую гиперплоскость.

```{r}
m1 <- ksvm(Colors ~ X1+X2, 
           data = train_data, 
           kernel = "splinedot", 
           C = 3)
m1
```

+ **Выбор ядра**: Использована модель опорных векторов (SVM) с ядром "splinedot". Выбор ядра оказывает значительное влияние на разделяющую гиперплоскость. Ядро "splinedot" может быть полезным при работе с нелинейными данными.

+ **Параметр C**: Значение параметра регуляризации C установлено на 3. Этот параметр контролирует штраф за ошибки классификации на обучающих данных. Значения C более высокие могут привести к более точной модели, но могут также вызвать переобучение.

+ **Количество опорных векторов**: Модель содержит 29 опорных векторов. Опорные векторы - это точки данных, которые оказывают влияние на определение разделяющей гиперплоскости. Меньшее количество опорных векторов может указывать на хорошую обобщающую способность модели, но слишком малое количество может свидетельствовать о недообучении, а слишком большое - о переобучении.

+ **Функция цели и ошибка обучения**: Значение целевой функции составляет -128.2091, а ошибка обучения на тренировочных данных составляет 13.33%. Целевая функция отражает оптимизируемый функционал, а ошибка обучения показывает процент ошибок модели на тренировочных данных. Ошибка обучения должна быть не слишком низкой (чтобы избежать переобучения) и не слишком высокой (чтобы модель была информативной).


#### Параметр kernel(различные ядра):

+ ***rbfdot*** Radial Basis kernel "Gaussian"

+ ***polydor*** Polynomial kernel

+ ***vanilladot*** Linear kernel

+ ***tanhdot*** Hyperbolic tangent kernel

+ ***laplacedot*** Laplacian kernel

+ ***besseldot*** Bessel kernel

+ ***anovadot*** ANOVA RBF kernel

+ ***splinedot*** Spline kernel
 
+ ***stringdot***  String kernel

Выбор ядра зависит от структуры и природы данных, поэтому обычно проводятся эксперименты с разными ядрами, чтобы определить, какое работает лучше для конкретного набора данных и задачи.

#### Прогнозы
Строим прогнозы для тестовой части выборки:
```{r}
table(predict(m1, test_data), test_data$Colors)
```

#### Выводы о производительности модели

```{r}
library(caret)

conf_mat <- confusionMatrix(data = predict(m1, test_data), reference = test_data$Colors)

conf_mat

```
Эта таблица (Confusion Matrix and Statistics) и метрики оценки модели позволяют сделать несколько выводов о производительности модели на тестовых данных:

+ ***Accuracy (Точность)***: 0.8 (или 80%) - Это общая точность модели, показывающая долю правильно классифицированных наблюдений относительно всех наблюдений.

+ ***95% CI (Интервал достоверности)***: Диапазон значений, в котором с 95% вероятностью лежит истинное значение Accuracy.

+ ***Kappa***: 0.6 - Это статистическая мера согласованности между предсказанными и наблюдаемыми классами, учитывающая случайную согласованность.

+ ***Sensitivity (Чувствительность)***: 0.8167 (или 81.67%) - Это способность модели правильно определять положительные случаи из всех истинных положительных случаев.

+ ***Specificity (Специфичность)***: 0.7833 (или 78.33%) - Это способность модели правильно определять отрицательные случаи из всех истинных отрицательных случаев.

+ ***Pos Pred Value*** (Положительное предсказательное значение): 0.7903 (или 79.03%) - Доля правильно предсказанных положительных случаев относительно всех предсказанных положительных случаев.

+ ***Neg Pred Value (Отрицательное предсказательное значение)***: 0.8103 (или 81.03%) - Доля правильно предсказанных отрицательных случаев относительно всех предсказанных отрицательных случаев.

+ ***Prevalence (Распространенность)***: 0.5 (или 50%) - Доля положительных случаев в тестовой выборке.

+ ***Detection Rate (Уровень обнаружения)***: 0.4083 (или 40.83%) - Доля правильно обнаруженных положительных случаев относительно всех истинных положительных случаев.

+ ***Balanced Accuracy (Сбалансированная точность)***: 0.8 (или 80%) - Среднее арифметическое между Sensitivity и Specificity, учитывающее дисбаланс классов.



#### Графики
Красивые графики, как мне кажется, можно получить только для случая двух объясняющих переменных. Для более, чем двух можно рисовать проекции. Закрашенные треугольники и кружочки — это опорные вектора, т.е. те точки, которые оказались на границе разделяющей полосы в спрямляющем пространстве.
```{r}
plot(m1, data = train_data)
```


Во многих моделях есть параметр, отвечающий за простоту модели. Чем проще модель, тем хуже модель описывает выборку, по которой она оценивалась. В методе опорных векторов ($SVM$) параметры, такие как параметр регуляризации $C$ и параметр ядра (например, $\sigma$ для гауссовского ядра), играют роль в настройке сложности модели. Увеличение параметра $C$ может привести к более сложной модели, склонной к переобучению, в то время как уменьшение $C$ может способствовать более простой модели, склонной к недообучению.<br>

Есть и универсальный способ выбора сложности модели — ***кросс валидация*** (перекрёстная проверка, cross validation). Её идея состоит в том, что надо оценивать качество прогнозов не по той же выборке, на которой оценивалась модель, а на новых наблюдениях.<br>


#### **k-кратная кросс-валидация**

***Кросс-валидация*** - это метод оценки производительности модели, который включает разделение доступных данных на несколько частей. Затем модель обучается на части данных и тестируется на оставшихся данных. Этот процесс повторяется несколько раз, каждый раз с разным набором данных для обучения и тестирования:

+ **Разделение на части**: Исходные данные разбиваются на несколько частей, называемых фолдами. Это гарантирует, что каждый фолд содержит подобное количество данных и представляет всю выборку.

+ **Обучение и тестирование**: Модель обучается на одном фолде и тестируется на остальных. Это повторяется для каждого фолда: каждый раз один фолд используется для тестирования, а остальные для обучения модели.

+ **Получение прогнозов**: Для каждого тестового фолда модель делает прогнозы. Таким образом, мы получаем прогнозы для всей выборки, однако каждая точка данных была использована только для тестирования один раз.

+ **Оценка качества модели**: Метрика оценки качества модели, такая как средняя ошибка или точность предсказаний, вычисляется для каждого тестового фолда, используя его прогнозы.

+ **Агрегация результатов**: Оценки качества модели для каждого фолда усредняются или агрегируются, чтобы получить окончательную оценку производительности модели.

+ **Оценка обобщающей способности**: Кросс-валидация помогает оценить, насколько модель может обобщать свои знания на новые данные, и позволяет избежать слишком сильного запоминания обучающих данных (переобучения).


#### **Популярные значения k:**

+ **10-кратная кроссвалидация**.
+ **k равно числу наблюдений**(***Leave-One-Out*** кросс-валидация (LOOCV)). Т.е. модель оценивается по всем наблюдениям кроме одного. Для этого невключенного наблюдения считается ошибка прогноза. Исключая по очереди то одно, то другое наблюдение, получаем ошибку прогноза для каждого наблюдения. По этим ошибкам считаем сумму квадратов. При этом подходе алгоритм не является случайным и зачастую есть готовые формулы для суммы квадратов ошибок прогнозов.


Рассмртрим пакет **caret**. В нем реализовано и деление выборки на тестовую и обучающую части и подбор параметров с помощью кросс-валидации для многих методов, в том числе для SVM.

```{r}
train_data$Colors <- make.names(train_data$Colors)

library(caret)
ctrl <- trainControl(classProbs = TRUE)
fit <- train(Colors ~ X1+X2, 
             data = train_data, 
             method = "svmRadial",
             trControl = ctrl)
fit
```
Это результаты кросс-валидации для подбора параметров модели метода опорных векторов с ядром радиальной базисной функции (RBF).

+ ***Support Vector Machines with Radial Basis Function Kernel***: Это сообщает о том, что использовалась модель метода опорных векторов с радиальным базисным ядром.

+ ***120 samples, 2 predictor, 2 classes: 'X0', 'X1'***: Это указывает на количество образцов (120), количество предикторов (2) и количество классов (2), которые модель предсказывает - 'X0' и 'X1'.

+ ***Resampling: Bootstrapped (25 reps)***: Процесс перекрестной проверки был выполнен методом бутстрэпа с 25 повторениями.

+ ***Resampling results across tuning parameters***: Это таблица, показывающая метрики качества модели (Accuracy и Kappa) для разных значений параметра регуляризации C. Для каждого значения C (0.25, 0.5, 1.0) приведены значения точности (Accuracy) и коэффициента Каппа (Kappa).

+ ***Tuning parameter 'sigma' was held constant at a value of 1.602763***: Это сообщает, что параметр 'sigma' был удержан на постоянном значении 1.602763 во время кросс-валидации.

+ ***Accuracy was used to select the optimal model using the largest value***: Отмечается, что для выбора оптимальной модели использовалась метрика Accuracy (точность), и была выбрана модель с наибольшим значением этой метрики.

+ ***The final values used for the model were sigma = 1.602763 and C = 1***: Значения параметров, которые были выбраны как оптимальные для модели после кросс-валидации: sigma = 1.602763 и C = 1.


#### Прогнозы 
Снова строим прогнозы для тестовой части выборки:

```{r}
table(predict(fit, test_data), test_data$Colors)
```




